{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27e21cb",
   "metadata": {},
   "source": [
    "# Pipeline Evaluation â€” Annotation Player\n",
    "\n",
    "**Listen to classified segments and assign ground-truth labels** for evaluating the full HindiBabyNet pipeline.\n",
    "\n",
    "Reads `classified_segments.parquet` from Stage 06 and slices audio from the analysis WAV in `audio_processed/`.\n",
    "\n",
    "### Workflow\n",
    "1. **Cell 2** â€” Configure participant, run ID, sampling\n",
    "2. **Cell 3** â€” Load parquet + analysis WAV, sample segments\n",
    "3. **Cell 4** â€” Annotate: plays each segment, type a shortcut + Enter\n",
    "4. **Cell 5** â€” Summary & quick evaluation\n",
    "\n",
    "### Label shortcuts\n",
    "\n",
    "| Key | Label | Description |\n",
    "|-----|-------|-------------|\n",
    "| `m` | `MAL` | Adult male speech |\n",
    "| `f` | `FEM` | Adult female speech |\n",
    "| `c` | `KCHI` | Target child vocalisation |\n",
    "| `o` | `OCH` | Other child |\n",
    "| `b` | `SIL` | Background / noise / silence |\n",
    "| `u` | `OVL` | Unclear / too ambiguous to label |\n",
    "\n",
    "| Key | Action |\n",
    "|-----|--------|\n",
    "| `r` | Replay current segment |\n",
    "| `p` | Go back (previous segment) |\n",
    "| `q` | Save & stop (resume later) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea7635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PARTICIPANT_ID = \"ABAN141223\"       # â† change this\n",
    "\n",
    "# Run ID â€” leave empty to auto-discover latest\n",
    "RUN_ID = \"\"  # e.g. \"20260217_133307\"\n",
    "\n",
    "# Paths\n",
    "ARTIFACTS_ROOT    = \"../artifacts/runs\"\n",
    "AUDIO_PROCESSED   = \"/scratch/users/arunps/hindibabynet/audio_processed\"\n",
    "ANNOTATION_ROOT   = \"/scratch/users/arunps/hindibabynet/annotations\"\n",
    "\n",
    "# â”€â”€ Sampling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Set to 0 to annotate ALL segments (warning: can be thousands)\n",
    "SAMPLE_PER_CLASS = 50     # random segments per predicted class\n",
    "SEED             = 42     # for reproducibility\n",
    "MIN_DURATION     = 0.3    # skip very short segments (seconds)\n",
    "MAX_DURATION     = 0.0    # 0 = no upper limit\n",
    "MIN_CONFIDENCE   = 0.0    # 0 = include all confidence levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# â”€â”€ All valid labels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PIPELINE_CLASSES = [\"MAL\", \"FEM\", \"KCHI\", \"SIL\"]\n",
    "ALL_LABELS = PIPELINE_CLASSES + [\"OCH\", \"OVL\"]\n",
    "\n",
    "SHORTCUT_MAP = {\n",
    "    \"m\": \"MAL\",\n",
    "    \"f\": \"FEM\",\n",
    "    \"c\": \"KCHI\",\n",
    "    \"o\": \"OCH\",\n",
    "    \"b\": \"SIL\",\n",
    "    \"u\": \"OVL\",\n",
    "}\n",
    "SHORTCUT_HELP = \"  \".join(f\"{k}={v}\" for k, v in SHORTCUT_MAP.items())\n",
    "\n",
    "# â”€â”€ Discover latest run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def find_latest_run(artifacts_root: str) -> str:\n",
    "    root = Path(artifacts_root)\n",
    "    runs = sorted(p.name for p in root.iterdir() if p.is_dir())\n",
    "    assert runs, f\"No runs found in {artifacts_root}\"\n",
    "    return runs[-1]\n",
    "\n",
    "# â”€â”€ Resolve paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "arts = Path(ARTIFACTS_ROOT)\n",
    "run_id = RUN_ID or find_latest_run(ARTIFACTS_ROOT)\n",
    "run_dir = arts / run_id\n",
    "\n",
    "# Classified segments parquet\n",
    "pq_path = run_dir / \"speaker_classification\" / f\"{PARTICIPANT_ID}_classified_segments.parquet\"\n",
    "assert pq_path.exists(), f\"Parquet not found: {pq_path}\"\n",
    "\n",
    "# Analysis WAV\n",
    "wav_path = Path(AUDIO_PROCESSED) / PARTICIPANT_ID / f\"{PARTICIPANT_ID}.wav\"\n",
    "assert wav_path.exists(), f\"Analysis WAV not found: {wav_path}\"\n",
    "\n",
    "# Annotation CSV (persisted between sessions)\n",
    "ann_dir = Path(ANNOTATION_ROOT) / PARTICIPANT_ID\n",
    "ann_dir.mkdir(parents=True, exist_ok=True)\n",
    "ann_csv = ann_dir / f\"{PARTICIPANT_ID}_eval_annotations.csv\"\n",
    "\n",
    "print(f\"Run ID         : {run_id}\")\n",
    "print(f\"Participant    : {PARTICIPANT_ID}\")\n",
    "print(f\"Parquet        : {pq_path}\")\n",
    "print(f\"Analysis WAV   : {wav_path}\")\n",
    "print(f\"Annotation CSV : {ann_csv}\")\n",
    "\n",
    "# â”€â”€ Load parquet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_all = pd.read_parquet(pq_path)\n",
    "print(f\"\\nTotal classified segments: {len(df_all)}\")\n",
    "print(f\"Class distribution:\")\n",
    "for cls in PIPELINE_CLASSES:\n",
    "    n = (df_all[\"predicted_class\"] == cls).sum()\n",
    "    print(f\"  {cls:<15}: {n}\")\n",
    "\n",
    "# â”€â”€ Filter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "mask = df_all[\"duration_sec\"] >= MIN_DURATION\n",
    "if MAX_DURATION > 0:\n",
    "    mask &= df_all[\"duration_sec\"] <= MAX_DURATION\n",
    "if MIN_CONFIDENCE > 0:\n",
    "    mask &= df_all[\"predicted_confidence\"] >= MIN_CONFIDENCE\n",
    "df_filtered = df_all[mask].reset_index(drop=True)\n",
    "print(f\"\\nAfter filters (min_dur={MIN_DURATION}s, max_dur={MAX_DURATION or 'none'}, \"\n",
    "      f\"min_conf={MIN_CONFIDENCE}): {len(df_filtered)}\")\n",
    "\n",
    "# â”€â”€ Sample â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if SAMPLE_PER_CLASS > 0:\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    sampled = []\n",
    "    for cls in PIPELINE_CLASSES:\n",
    "        cls_df = df_filtered[df_filtered[\"predicted_class\"] == cls]\n",
    "        n = min(SAMPLE_PER_CLASS, len(cls_df))\n",
    "        if n > 0:\n",
    "            idx = rng.choice(len(cls_df), size=n, replace=False)\n",
    "            sampled.append(cls_df.iloc[idx])\n",
    "            print(f\"  Sampled {cls}: {n}/{len(cls_df)}\")\n",
    "        else:\n",
    "            print(f\"  Sampled {cls}: 0 (none available)\")\n",
    "    df_sample = pd.concat(sampled, ignore_index=True).sort_values(\"start_sec\").reset_index(drop=True)\n",
    "else:\n",
    "    df_sample = df_filtered.sort_values(\"start_sec\").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nSegments to annotate: {len(df_sample)}\")\n",
    "\n",
    "# â”€â”€ Load existing annotations if resuming â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if ann_csv.exists():\n",
    "    df_prev = pd.read_csv(ann_csv)\n",
    "    # Merge previous human_label into df_sample by matching start_sec + end_sec\n",
    "    if \"human_label\" in df_prev.columns:\n",
    "        prev_map = {\n",
    "            (round(r[\"start_sec\"], 4), round(r[\"end_sec\"], 4)): r[\"human_label\"]\n",
    "            for _, r in df_prev.iterrows()\n",
    "            if pd.notna(r.get(\"human_label\")) and str(r[\"human_label\"]).strip()\n",
    "        }\n",
    "        df_sample[\"human_label\"] = df_sample.apply(\n",
    "            lambda r: prev_map.get((round(r[\"start_sec\"], 4), round(r[\"end_sec\"], 4)), \"\"),\n",
    "            axis=1,\n",
    "        )\n",
    "        n_restored = (df_sample[\"human_label\"].str.strip() != \"\").sum()\n",
    "        if n_restored > 0:\n",
    "            print(f\"Restored {n_restored} previous annotations\")\n",
    "    else:\n",
    "        df_sample[\"human_label\"] = \"\"\n",
    "else:\n",
    "    df_sample[\"human_label\"] = \"\"\n",
    "\n",
    "# Add notes column if missing\n",
    "if \"notes\" not in df_sample.columns:\n",
    "    df_sample[\"notes\"] = \"\"\n",
    "\n",
    "# â”€â”€ Load analysis WAV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(f\"\\nLoading analysis WAV...\")\n",
    "audio, sr = sf.read(str(wav_path), dtype=\"float32\")\n",
    "if audio.ndim > 1:\n",
    "    audio = audio.mean(axis=1)\n",
    "print(f\"Audio: {len(audio)/sr:.1f}s ({len(audio)/sr/3600:.2f}h) @ {sr} Hz\")\n",
    "print(f\"\\nâœ“ Ready â€” run the next cell to start annotating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Annotate: listen + label â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Re-run this cell to resume from where you left off.\n",
    "\n",
    "total = len(df_sample)\n",
    "\n",
    "def _first_unlabelled():\n",
    "    for i in range(total):\n",
    "        val = df_sample.at[i, \"human_label\"]\n",
    "        if pd.isna(val) or str(val).strip() == \"\":\n",
    "            return i\n",
    "    return total\n",
    "\n",
    "start_idx = _first_unlabelled()\n",
    "if start_idx >= total:\n",
    "    print(f\"All {total} segments already annotated!\")\n",
    "    print(df_sample[\"human_label\"].value_counts().to_string())\n",
    "    print(f\"\\nRun the next cell for summary.\")\n",
    "\n",
    "def _save():\n",
    "    df_sample.to_csv(ann_csv, index=False)\n",
    "\n",
    "def _progress_str():\n",
    "    done = df_sample[\"human_label\"].notna() & (df_sample[\"human_label\"].str.strip() != \"\")\n",
    "    counts = df_sample.loc[done, \"human_label\"].value_counts()\n",
    "    parts = [f\"{k}={v}\" for k, v in sorted(counts.items())]\n",
    "    return f\"{done.sum()}/{total}  |  \" + \"  \".join(parts)\n",
    "\n",
    "def _slice_audio(start_sec, end_sec):\n",
    "    s = max(0, int(round(start_sec * sr)))\n",
    "    e = min(len(audio), int(round(end_sec * sr)))\n",
    "    return audio[s:e]\n",
    "\n",
    "print(f\"Progress: {_progress_str()}\")\n",
    "print(f\"Starting from segment {start_idx + 1}\\n\")\n",
    "print(f\"Shortcuts: {SHORTCUT_HELP}\")\n",
    "print(f\"Actions:   r=replay  p=previous  q=save & stop\\n\")\n",
    "\n",
    "idx = start_idx\n",
    "try:\n",
    "    while 0 <= idx < total:\n",
    "        row = df_sample.iloc[idx]\n",
    "        pred_cls = row[\"predicted_class\"]\n",
    "        pred_conf = row[\"predicted_confidence\"]\n",
    "        dur = row[\"duration_sec\"]\n",
    "        start_s = row[\"start_sec\"]\n",
    "        end_s = row[\"end_sec\"]\n",
    "        existing = row[\"human_label\"]\n",
    "        has_label = pd.notna(existing) and str(existing).strip() != \"\"\n",
    "        tag = f\"  [labeled: {existing}]\" if has_label else \"\"\n",
    "\n",
    "        chunk = _slice_audio(start_s, end_s)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"[{idx+1}/{total}]  {start_s:.2f}s â€“ {end_s:.2f}s  ({dur:.2f}s)\")\n",
    "        print(f\"  Predicted: {pred_cls}  (conf={pred_conf:.3f}){tag}\")\n",
    "        print(f\"  Progress: {_progress_str()}\")\n",
    "        print(\"â”€\" * 55)\n",
    "        print(f\"  {SHORTCUT_HELP}\")\n",
    "        print(f\"  r=replay  p=previous  q=save & stop\")\n",
    "\n",
    "        display(ipd.Audio(chunk, rate=sr, autoplay=True))\n",
    "\n",
    "        while True:\n",
    "            raw = input(\"Label: \").strip().lower()\n",
    "            if raw == \"q\":\n",
    "                _save()\n",
    "                clear_output(wait=True)\n",
    "                print(f\"âœ“ Saved. {_progress_str()}\")\n",
    "                print(f\"  CSV: {ann_csv}\")\n",
    "                print(f\"  Re-run this cell to resume.\")\n",
    "                raise StopIteration\n",
    "            elif raw == \"r\":\n",
    "                clear_output(wait=True)\n",
    "                print(f\"[{idx+1}/{total}]  (replaying...)  Predicted: {pred_cls}\")\n",
    "                print(\"â”€\" * 55)\n",
    "                display(ipd.Audio(chunk, rate=sr, autoplay=True))\n",
    "                continue\n",
    "            elif raw == \"p\":\n",
    "                if idx > 0:\n",
    "                    idx -= 1\n",
    "                break\n",
    "            elif raw in SHORTCUT_MAP:\n",
    "                label = SHORTCUT_MAP[raw]\n",
    "                df_sample.at[idx, \"human_label\"] = label\n",
    "                idx += 1\n",
    "                done_count = (df_sample[\"human_label\"].notna() & (df_sample[\"human_label\"].str.strip() != \"\")).sum()\n",
    "                if done_count % 10 == 0:\n",
    "                    _save()\n",
    "                break\n",
    "            else:\n",
    "                print(f\"  Invalid. Shortcuts: {SHORTCUT_HELP}\")\n",
    "\n",
    "    _save()\n",
    "    clear_output(wait=True)\n",
    "    print(f\"ðŸŽ‰ All {total} segments annotated!\")\n",
    "    print(f\"  {_progress_str()}\")\n",
    "    print(f\"  CSV: {ann_csv}\")\n",
    "    print(f\"\\nRun the next cell for summary & evaluation.\")\n",
    "\n",
    "except StopIteration:\n",
    "    pass\n",
    "except KeyboardInterrupt:\n",
    "    _save()\n",
    "    print(f\"\\nInterrupted â€” saved. Re-run to resume.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Summary & quick evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "df_fresh = pd.read_csv(ann_csv)\n",
    "done = df_fresh[\"human_label\"].notna() & (df_fresh[\"human_label\"].str.strip() != \"\")\n",
    "annotated = df_fresh[done].copy()\n",
    "\n",
    "print(f\"Annotation CSV : {ann_csv}\")\n",
    "print(f\"Annotated      : {len(annotated)}/{len(df_fresh)}\\n\")\n",
    "\n",
    "if len(annotated) == 0:\n",
    "    print(\"No annotations yet â€” run the annotation cell first.\")\n",
    "else:\n",
    "    # Label distribution\n",
    "    print(\"â”€â”€ Human label distribution â”€â”€\")\n",
    "    for label, count in annotated[\"human_label\"].value_counts().items():\n",
    "        print(f\"  {label:<15}: {count}\")\n",
    "\n",
    "    # Agreement with predictions (4-class pipeline labels only)\n",
    "    eval_df = annotated[annotated[\"human_label\"].isin(PIPELINE_CLASSES)].copy()\n",
    "    if len(eval_df) > 0:\n",
    "        correct = (eval_df[\"human_label\"] == eval_df[\"predicted_class\"]).sum()\n",
    "        acc = correct / len(eval_df)\n",
    "        print(f\"\\nâ”€â”€ Pipeline agreement (4-class) â”€â”€\")\n",
    "        print(f\"  Accuracy: {correct}/{len(eval_df)} = {acc:.1%}\")\n",
    "\n",
    "        # Per-class accuracy\n",
    "        print(f\"\\n  Per-class accuracy:\")\n",
    "        for cls in PIPELINE_CLASSES:\n",
    "            cls_df = eval_df[eval_df[\"predicted_class\"] == cls]\n",
    "            if len(cls_df) > 0:\n",
    "                cls_correct = (cls_df[\"human_label\"] == cls).sum()\n",
    "                print(f\"    {cls:<15}: {cls_correct}/{len(cls_df)} = {cls_correct/len(cls_df):.1%}\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        print(f\"\\nâ”€â”€ Confusion matrix (rows=true, cols=predicted) â”€â”€\")\n",
    "        labels_in = sorted(set(eval_df[\"predicted_class\"]) | set(eval_df[\"human_label\"]))\n",
    "        from sklearn.metrics import confusion_matrix, recall_score\n",
    "        cm = confusion_matrix(eval_df[\"human_label\"], eval_df[\"predicted_class\"], labels=labels_in)\n",
    "        cm_df = pd.DataFrame(cm, index=[f\"true:{l}\" for l in labels_in],\n",
    "                             columns=[f\"pred:{l}\" for l in labels_in])\n",
    "        display(cm_df)\n",
    "\n",
    "        # UAR\n",
    "        uar = recall_score(eval_df[\"human_label\"], eval_df[\"predicted_class\"],\n",
    "                           labels=PIPELINE_CLASSES, average=\"macro\", zero_division=0)\n",
    "        print(f\"\\n  UAR (macro recall): {uar:.4f} ({uar*100:.1f}%)\")\n",
    "\n",
    "    # Child sub-classification stats\n",
    "    n_kchi = (annotated[\"human_label\"] == \"KCHI\").sum()\n",
    "    n_och = (annotated[\"human_label\"] == \"OCH\").sum()\n",
    "    n_ovl = (annotated[\"human_label\"] == \"OVL\").sum()\n",
    "    if n_kchi + n_och > 0:\n",
    "        print(f\"\\nâ”€â”€ Child sub-classification â”€â”€\")\n",
    "        print(f\"  KCHI (target)   : {n_kchi}\")\n",
    "        print(f\"  OCH  (other)    : {n_och}\")\n",
    "    if n_ovl > 0:\n",
    "        print(f\"\\n  OVL (unclear)   : {n_ovl}\")\n",
    "\n",
    "    # Confidence analysis\n",
    "    if \"predicted_confidence\" in eval_df.columns and len(eval_df) > 0:\n",
    "        eval_df = eval_df.copy()\n",
    "        eval_df[\"correct\"] = eval_df[\"human_label\"] == eval_df[\"predicted_class\"]\n",
    "        mean_correct = eval_df.loc[eval_df[\"correct\"], \"predicted_confidence\"].mean()\n",
    "        mean_wrong = eval_df.loc[~eval_df[\"correct\"], \"predicted_confidence\"].mean()\n",
    "        print(f\"\\nâ”€â”€ Confidence analysis â”€â”€\")\n",
    "        print(f\"  Mean confidence (correct)  : {mean_correct:.4f}\")\n",
    "        if pd.notna(mean_wrong):\n",
    "            print(f\"  Mean confidence (incorrect): {mean_wrong:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HindiBabyNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
