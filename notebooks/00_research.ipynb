{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f07b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available(), torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03885ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import webrtcvad\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_id = \"ABAN141223\"\n",
    "session_date   = \"20250216\"\n",
    "\n",
    "session_dir = Path(\"/scratch/users/arunps/hindibabynet/audio_raw/ABAN141223/20250216\")\n",
    "wav_files = sorted(session_dir.glob(\"*.WAV\")) + sorted(session_dir.glob(\"*.wav\"))\n",
    "\n",
    "wav_path = Path(wav_files[0])  # choose which file\n",
    "info = sf.info(str(wav_path))\n",
    "print(wav_path.name, info.samplerate, info.channels, info.duration/3600, \"hours\")\n",
    "recording_id = wav_path.stem\n",
    "\n",
    "# diarization bounds\n",
    "MIN_SPEAKERS = 2\n",
    "MAX_SPEAKERS = 4\n",
    "\n",
    "# chunking\n",
    "CHUNK_SEC   = 15 * 60  # 15 min\n",
    "OVERLAP_SEC = 10       # overlap between chunks to avoid boundary issues\n",
    "\n",
    "# VAD params\n",
    "VAD_AGGR = 2\n",
    "VAD_FRAME_MS = 30\n",
    "VAD_MIN_REGION_MS = 300\n",
    "\n",
    "# intersection / post-filter\n",
    "MIN_KEEP_SEC = 0.20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e307ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for p in wav_files:\n",
    "    info = sf.info(str(p))\n",
    "    rows.append({\n",
    "        \"participant_id\": participant_id,\n",
    "        \"session_date\": session_date,\n",
    "        \"recording_id\": p.stem,\n",
    "        \"path\": str(p),\n",
    "        \"duration_sec\": float(info.duration),\n",
    "        \"sample_rate\": int(info.samplerate),\n",
    "        \"channels\": int(info.channels),\n",
    "        \"size_bytes\": p.stat().st_size,\n",
    "    })\n",
    "\n",
    "recordings = pd.DataFrame(rows)\n",
    "recordings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "assert os.getenv(\"HF_TOKEN\") is not None, \"HF_TOKEN not loaded\"\n",
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYANNOTE_DISABLE_NOTEBOOK\"] = \"1\"\n",
    "\n",
    "scratch_cache = f\"/scratch/users/{os.environ['USER']}/.cache/huggingface\"\n",
    "os.environ[\"HF_HOME\"] = scratch_cache\n",
    "os.environ[\"HF_HUB_CACHE\"] = f\"{scratch_cache}/hub\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = f\"{scratch_cache}/transformers\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "pipeline.to(device)\n",
    "print(\"Diarization pipeline loaded on\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0020585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(duration_sec: float, chunk_sec: float, overlap_sec: float):\n",
    "    \"\"\"Yield (chunk_id, chunk_start, chunk_end) with overlap.\"\"\"\n",
    "    step = chunk_sec - overlap_sec\n",
    "    assert step > 0, \"chunk_sec must be > overlap_sec\"\n",
    "\n",
    "    t = 0.0\n",
    "    chunk_id = 0\n",
    "    while t < duration_sec:\n",
    "        s = t\n",
    "        e = min(t + chunk_sec, duration_sec)\n",
    "        yield chunk_id, s, e\n",
    "        if e >= duration_sec:\n",
    "            break\n",
    "        t += step\n",
    "        chunk_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e44d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webrtc_vad_regions_streaming(\n",
    "    path: Path,\n",
    "    aggressiveness: int = 2,\n",
    "    frame_ms: int = 30,\n",
    "    min_region_ms: int = 300,\n",
    "):\n",
    "    vad = webrtcvad.Vad(aggressiveness)\n",
    "    info = sf.info(str(path))\n",
    "    sr = info.samplerate\n",
    "\n",
    "    if sr not in (8000, 16000, 32000, 48000):\n",
    "        raise ValueError(f\"webrtcvad needs sr in (8k,16k,32k,48k). got: {sr}\")\n",
    "\n",
    "    frame_len = int(sr * frame_ms / 1000)\n",
    "\n",
    "    speech_flags = []\n",
    "    with sf.SoundFile(str(path), mode=\"r\") as f:\n",
    "        while True:\n",
    "            frame = f.read(frames=frame_len, dtype=\"int16\", always_2d=True)\n",
    "            if frame.size == 0 or len(frame) < frame_len:\n",
    "                break\n",
    "            mono = frame[:, 0]  # channel 0\n",
    "            speech_flags.append(vad.is_speech(mono.tobytes(), sr))\n",
    "\n",
    "    # merge consecutive True flags\n",
    "    regions = []\n",
    "    in_speech = False\n",
    "    start_i = 0\n",
    "    for i, is_speech in enumerate(speech_flags):\n",
    "        if is_speech and not in_speech:\n",
    "            in_speech = True\n",
    "            start_i = i\n",
    "        elif (not is_speech) and in_speech:\n",
    "            in_speech = False\n",
    "            regions.append((start_i, i))\n",
    "    if in_speech:\n",
    "        regions.append((start_i, len(speech_flags)))\n",
    "\n",
    "    out = []\n",
    "    for s_i, e_i in regions:\n",
    "        s = (s_i * frame_len) / sr\n",
    "        e = (e_i * frame_len) / sr\n",
    "        if (e - s) * 1000 >= min_region_ms:\n",
    "            out.append((float(s), float(e)))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74406e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = sf.info(str(wav_path))\n",
    "full_duration = float(info.duration)\n",
    "\n",
    "vad_intervals = webrtc_vad_regions_streaming(\n",
    "    wav_path,\n",
    "    aggressiveness=VAD_AGGR,\n",
    "    frame_ms=VAD_FRAME_MS,\n",
    "    min_region_ms=VAD_MIN_REGION_MS\n",
    ")\n",
    "\n",
    "print(\"Full duration (hours):\", full_duration / 3600)\n",
    "print(\"VAD intervals:\", len(vad_intervals), \"first:\", vad_intervals[:3])\n",
    "\n",
    "vad_df = (\n",
    "    pd.DataFrame([{\n",
    "        \"participant_id\": participant_id,\n",
    "        \"session_date\": session_date,\n",
    "        \"recording_id\": recording_id,\n",
    "        \"wav_path\": str(wav_path),\n",
    "        \"start_sec\": s,\n",
    "        \"end_sec\": e,\n",
    "        \"duration_sec\": e - s,\n",
    "    } for s, e in vad_intervals])\n",
    "    .sort_values([\"start_sec\", \"end_sec\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "vad_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def write_wav_chunk(\n",
    "    wav_path: Path,\n",
    "    chunk_path: Path,\n",
    "    start_sec: float,\n",
    "    end_sec: float\n",
    "):\n",
    "    try:\n",
    "        info = sf.info(str(wav_path))\n",
    "        sr = info.samplerate\n",
    "        total_frames = info.frames\n",
    "\n",
    "        start_frame = max(0, int(start_sec * sr))\n",
    "        end_frame   = min(total_frames, int(end_sec * sr))\n",
    "        n_frames    = end_frame - start_frame\n",
    "\n",
    "        # ZERO-SAMPLE GUARD\n",
    "        if n_frames <= 0:\n",
    "            return None\n",
    "\n",
    "        audio, _ = sf.read(\n",
    "            str(wav_path),\n",
    "            start=start_frame,\n",
    "            frames=n_frames,\n",
    "            dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # EMPTY ARRAY GUARD\n",
    "        if audio is None or audio.size == 0:\n",
    "            return None\n",
    "\n",
    "        chunk_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        sf.write(\n",
    "            str(chunk_path),\n",
    "            audio,\n",
    "            sr,\n",
    "            format=\"WAV\",\n",
    "            subtype=\"PCM_16\"\n",
    "        )\n",
    "\n",
    "        return chunk_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] write_wav_chunk failed ({chunk_path.name}): {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "all_turn_rows = []\n",
    "\n",
    "tmp_chunks_dir = Path(\"/scratch/users\") / Path.home().name / \"hindibabynet_tmp_chunks\"\n",
    "tmp_chunks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for chunk_id, chunk_start, chunk_end in make_chunks(full_duration, CHUNK_SEC, OVERLAP_SEC):\n",
    "\n",
    "    chunk_wav = tmp_chunks_dir / f\"{wav_path.stem}_chunk{chunk_id:04d}_{int(chunk_start)}_{int(chunk_end)}.wav\"\n",
    "\n",
    "    # skip if zero samples / empty / write failed\n",
    "    out = write_wav_chunk(wav_path, chunk_wav, chunk_start, chunk_end)\n",
    "    if out is None:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        diar_chunk = pipeline(\n",
    "            {\"audio\": str(chunk_wav)},\n",
    "            min_speakers=MIN_SPEAKERS,\n",
    "            max_speakers=MAX_SPEAKERS\n",
    "        )\n",
    "\n",
    "        for seg, _, spk in diar_chunk.itertracks(yield_label=True):\n",
    "            # convert chunk-local to GLOBAL times\n",
    "            s = float(seg.start) + float(chunk_start)\n",
    "            e = float(seg.end) + float(chunk_start)\n",
    "            if e <= s:\n",
    "                continue\n",
    "\n",
    "            all_turn_rows.append({\n",
    "                \"participant_id\": participant_id,\n",
    "                \"session_date\": session_date,\n",
    "                \"recording_id\": recording_id,\n",
    "                \"wav_path\": str(wav_path),\n",
    "                \"chunk_id\": int(chunk_id),\n",
    "                \"chunk_start_sec\": float(chunk_start),\n",
    "                \"chunk_end_sec\": float(chunk_end),\n",
    "                \"speaker_id_local\": spk,\n",
    "                \"start_sec\": s,\n",
    "                \"end_sec\": e,\n",
    "                \"duration_sec\": float(e - s),\n",
    "                \"chunk_wav_path\": str(chunk_wav),  \n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Diarization failed for chunk {chunk_id} ({chunk_wav.name}): {e}\")\n",
    "\n",
    "    finally:\n",
    "        # delete chunk immediately to avoid scratch filling up\n",
    "        try:\n",
    "            chunk_wav.unlink(missing_ok=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "print(\"Turns collected:\", len(all_turn_rows))\n",
    "\n",
    "turns_df = (\n",
    "    pd.DataFrame(all_turn_rows)\n",
    "      .sort_values([\"start_sec\", \"end_sec\"])\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "turns_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e807032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_turns_with_vad(turns_df: pd.DataFrame, vad_intervals, min_keep_sec: float = 0.0):\n",
    "    # ensure sorted\n",
    "    turns_df = turns_df.sort_values([\"start_sec\", \"end_sec\"]).reset_index(drop=True)\n",
    "    diar_arr = turns_df[[\"start_sec\", \"end_sec\", \"chunk_id\", \"speaker_id_local\"]].to_numpy()\n",
    "    vad_arr = np.array(sorted(vad_intervals), dtype=float)\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    rows = []\n",
    "\n",
    "    def intersect(a_s, a_e, b_s, b_e):\n",
    "        s = max(a_s, b_s)\n",
    "        e = min(a_e, b_e)\n",
    "        return (s, e) if s < e else None\n",
    "\n",
    "    while i < len(diar_arr) and j < len(vad_arr):\n",
    "        ds, de, cid, spk = diar_arr[i]\n",
    "        vs, ve = vad_arr[j]\n",
    "\n",
    "        inter = intersect(ds, de, vs, ve)\n",
    "        if inter is not None:\n",
    "            s, e = inter\n",
    "            dur = float(e - s)\n",
    "            if dur >= min_keep_sec:\n",
    "                rows.append({\n",
    "                    \"start_sec\": float(s),\n",
    "                    \"end_sec\": float(e),\n",
    "                    \"duration_sec\": dur,\n",
    "                    \"chunk_id\": int(cid),\n",
    "                    \"speaker_id_local\": str(spk),\n",
    "                })\n",
    "\n",
    "        if de <= ve:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb53dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_only_df = intersect_turns_with_vad(\n",
    "    turns_df=turns_df,\n",
    "    vad_intervals=vad_intervals,\n",
    "    min_keep_sec=MIN_KEEP_SEC\n",
    ")\n",
    "\n",
    "# attach metadata\n",
    "speech_only_df.insert(0, \"wav_path\", str(wav_path))\n",
    "speech_only_df.insert(0, \"recording_id\", recording_id)\n",
    "speech_only_df.insert(0, \"session_date\", session_date)\n",
    "speech_only_df.insert(0, \"participant_id\", participant_id)\n",
    "\n",
    "final_df_full = speech_only_df[[\n",
    "    \"participant_id\",\"session_date\",\"recording_id\",\"wav_path\",\n",
    "    \"chunk_id\",\"start_sec\",\"end_sec\",\"duration_sec\",\n",
    "    \"speaker_id_local\"\n",
    "]].sort_values([\"start_sec\",\"end_sec\"]).reset_index(drop=True)\n",
    "\n",
    "final_df_full.head(), len(final_df_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd63e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_full.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_full.groupby([\"chunk_id\", \"speaker_id_local\"])[\"duration_sec\"].sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d29866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dur = final_df_full[\"duration_sec\"].astype(float)\n",
    "\n",
    "summary = dur.describe(percentiles=[0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]).to_frame().T\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bcf3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.2, 0.3, 0.5, 0.8, 1.0, 2.0, 5.0]\n",
    "\n",
    "counts = []\n",
    "n = len(dur)\n",
    "for t in thresholds:\n",
    "    counts.append({\n",
    "        \"threshold_sec\": t,\n",
    "        \"n_segments\": int((dur < t).sum()),\n",
    "        \"pct_segments\": float((dur < t).mean() * 100),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 0.2, 0.5, 1, 2, 5, 10, np.inf]\n",
    "labels = [\"<0.2\", \"0.2-0.5\", \"0.5-1\", \"1-2\", \"2-5\", \"5-10\", \"10+\"]\n",
    "\n",
    "tmp = final_df_full.copy()\n",
    "tmp[\"dur_bin\"] = pd.cut(tmp[\"duration_sec\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "by_bin = (\n",
    "    tmp.groupby(\"dur_bin\")\n",
    "       .agg(\n",
    "           n_segments=(\"duration_sec\", \"size\"),\n",
    "           total_sec=(\"duration_sec\", \"sum\"),\n",
    "           mean_sec=(\"duration_sec\", \"mean\"),\n",
    "           median_sec=(\"duration_sec\", \"median\"),\n",
    "       )\n",
    "       .reset_index()\n",
    ")\n",
    "\n",
    "by_bin[\"pct_segments\"] = by_bin[\"n_segments\"] / by_bin[\"n_segments\"].sum() * 100\n",
    "by_bin[\"pct_time\"] = by_bin[\"total_sec\"] / by_bin[\"total_sec\"].sum() * 100\n",
    "\n",
    "by_bin.sort_values(\"dur_bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb45e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(dur, bins=80)\n",
    "plt.xlabel(\"duration_sec\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Diar∩VAD segment duration distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad40b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = (\n",
    "    final_df_full\n",
    "    .groupby([\"chunk_id\", \"speaker_id_local\"])[\"duration_sec\"]\n",
    "    .agg([\"count\", \"sum\", \"mean\", \"median\", \"max\"])\n",
    "    .sort_values(\"sum\", ascending=False)\n",
    ")\n",
    "\n",
    "grp.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986cafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = final_df_full.copy()\n",
    "\n",
    "overlap_rows = []\n",
    "\n",
    "for chunk_id, g in df.groupby(\"chunk_id\"):\n",
    "    g = g.sort_values(\"start_sec\").reset_index(drop=True)\n",
    "\n",
    "    for i in range(len(g)):\n",
    "        si, ei, spki = g.loc[i, [\"start_sec\", \"end_sec\", \"speaker_id_local\"]]\n",
    "\n",
    "        for j in range(i + 1, len(g)):\n",
    "            sj, ej, spkj = g.loc[j, [\"start_sec\", \"end_sec\", \"speaker_id_local\"]]\n",
    "\n",
    "            # stop early (sorted by start time)\n",
    "            if sj >= ei:\n",
    "                break\n",
    "\n",
    "            if spki != spkj:\n",
    "                overlap_start = max(si, sj)\n",
    "                overlap_end = min(ei, ej)\n",
    "\n",
    "                if overlap_end > overlap_start:\n",
    "                    overlap_rows.append({\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"speaker_1\": spki,\n",
    "                        \"speaker_2\": spkj,\n",
    "                        \"seg1_start\": si,\n",
    "                        \"seg1_end\": ei,\n",
    "                        \"seg2_start\": sj,\n",
    "                        \"seg2_end\": ej,\n",
    "                        \"overlap_start\": overlap_start,\n",
    "                        \"overlap_end\": overlap_end,\n",
    "                        \"overlap_dur\": overlap_end - overlap_start,\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a63a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_df = pd.DataFrame(overlap_rows)\n",
    "\n",
    "len(overlap_df), overlap_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b65f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_df[\"overlap_dur\"].describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa47ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_in_overlap = pd.concat([\n",
    "    overlap_df[[\"chunk_id\", \"speaker_1\", \"seg1_start\", \"seg1_end\"]]\n",
    "        .rename(columns={\"speaker_1\": \"speaker_id_local\",\n",
    "                          \"seg1_start\": \"start_sec\",\n",
    "                          \"seg1_end\": \"end_sec\"}),\n",
    "\n",
    "    overlap_df[[\"chunk_id\", \"speaker_2\", \"seg2_start\", \"seg2_end\"]]\n",
    "        .rename(columns={\"speaker_2\": \"speaker_id_local\",\n",
    "                          \"seg2_start\": \"start_sec\",\n",
    "                          \"seg2_end\": \"end_sec\"})\n",
    "]).drop_duplicates()\n",
    "\n",
    "len(segments_in_overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(segments_in_overlap) / len(df) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 0.2, 0.5, 1, 2, 5, 10]\n",
    "labels = [\"<0.2\", \"0.2-0.5\", \"0.5-1\", \"1-2\", \"2-5\", \"5+\"]\n",
    "\n",
    "overlap_df[\"dur_bin\"] = pd.cut(\n",
    "    overlap_df[\"overlap_dur\"],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    right=False\n",
    ")\n",
    "\n",
    "(\n",
    "    overlap_df\n",
    "    .groupby(\"dur_bin\")\n",
    "    .agg(\n",
    "        n_overlaps=(\"overlap_dur\", \"size\"),\n",
    "        total_overlap_sec=(\"overlap_dur\", \"sum\"),\n",
    "    )\n",
    "    .assign(\n",
    "        pct_overlaps=lambda x: x[\"n_overlaps\"] / x[\"n_overlaps\"].sum() * 100,\n",
    "        pct_time=lambda x: x[\"total_overlap_sec\"] / x[\"total_overlap_sec\"].sum() * 100,\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_df.sort_values(\"overlap_dur\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32587d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "from praatio import textgrid as tgio\n",
    "\n",
    "\n",
    "def _make_interval_tier(name, entries, xmin, xmax):\n",
    "    \"\"\"\n",
    "    Create IntervalTier across praatio versions.\n",
    "    \"\"\"\n",
    "    # IntervalTier(name, entries, minT, maxT)\n",
    "    try:\n",
    "        return tgio.IntervalTier(str(name), entries, xmin, xmax)\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    # IntervalTier(name, entries=..., minT=..., maxT=...)\n",
    "    try:\n",
    "        return tgio.IntervalTier(name=str(name), entries=entries, minT=xmin, maxT=xmax)\n",
    "    except TypeError:\n",
    "        pass\n",
    "\n",
    "    # IntervalTier(name, entryList=..., minT=..., maxT=...)\n",
    "    return tgio.IntervalTier(name=str(name), entryList=entries, minT=xmin, maxT=xmax)\n",
    "\n",
    "\n",
    "def df_to_textgrid_by_speaker(\n",
    "    df: pd.DataFrame,\n",
    "    wav_path: Path,\n",
    "    out_textgrid_path: Path,\n",
    "    start_col: str = \"start_sec\",\n",
    "    end_col: str = \"end_sec\",\n",
    "    speaker_col: str = \"speaker_id\",\n",
    "    label_col: str | None = None,  # None -> label = speaker_id\n",
    "):\n",
    "    wav_path = Path(wav_path)\n",
    "    out_textgrid_path = Path(out_textgrid_path)\n",
    "\n",
    "    info = sf.info(str(wav_path))\n",
    "    xmin = 0.0\n",
    "    xmax = float(info.duration)\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df[df[end_col] > df[start_col]].sort_values([speaker_col, start_col, end_col])\n",
    "\n",
    "    tg = tgio.Textgrid()\n",
    "    tg.minTimestamp = xmin\n",
    "    tg.maxTimestamp = xmax\n",
    "\n",
    "    for spk, g in df.groupby(speaker_col):\n",
    "        entries = []\n",
    "        for r in g.itertuples(index=False):\n",
    "            s = float(getattr(r, start_col))\n",
    "            e = float(getattr(r, end_col))\n",
    "\n",
    "            # clamp\n",
    "            s = max(xmin, min(s, xmax))\n",
    "            e = max(xmin, min(e, xmax))\n",
    "            if e <= s:\n",
    "                continue\n",
    "\n",
    "            lab = str(spk) if label_col is None else str(getattr(r, label_col))\n",
    "            entries.append((s, e, lab))\n",
    "\n",
    "        # ensure non-overlap within the speaker tier\n",
    "        entries.sort(key=lambda x: (x[0], x[1]))\n",
    "        cleaned = []\n",
    "        last_end = -1.0\n",
    "        for s, e, lab in entries:\n",
    "            if s < last_end:\n",
    "                s = last_end\n",
    "            if e > s:\n",
    "                cleaned.append((s, e, lab))\n",
    "                last_end = e\n",
    "\n",
    "        tier = _make_interval_tier(str(spk), cleaned, xmin, xmax)\n",
    "        tg.addTier(tier)\n",
    "\n",
    "    tg.save(str(out_textgrid_path), format=\"short_textgrid\", includeBlankSpaces=True)\n",
    "    return out_textgrid_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da518ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tmp_dir = Path(\"/scratch/users\") / Path.home().name / \"hindibabynet_tmp\"\n",
    "tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "tg_path = df_to_textgrid_by_speaker(\n",
    "    df=final_df_full,                         # diarization ∩ VAD DataFrame\n",
    "    wav_path=wav_path,                   # full audio file\n",
    "    out_textgrid_path=tmp_dir / f\"{wav_path.stem}.TextGrid\",\n",
    ")\n",
    "\n",
    "tg_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcebb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# CLEANUP: remove all chunk WAVs at once\n",
    "# --------------------------\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "tmp_chunks_dir = Path(\"/scratch/users\") / Path.home().name / \"hindibabynet_tmp_chunks\"\n",
    "\n",
    "if tmp_chunks_dir.exists():\n",
    "    shutil.rmtree(tmp_chunks_dir)\n",
    "    print(f\"Deleted temporary chunk directory: {tmp_chunks_dir}\")\n",
    "else:\n",
    "    print(\"No temporary chunk directory found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_close_segments(\n",
    "    df: pd.DataFrame,\n",
    "    gap_thresh: float = 0.5\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # keep original row index for traceability\n",
    "    df[\"_orig_row\"] = df.index\n",
    "\n",
    "    # ensure deterministic ordering\n",
    "    sort_cols = [\n",
    "        \"participant_id\", \"session_date\", \"recording_id\", \"wav_path\",\n",
    "        \"chunk_id\", \"speaker_id_local\", \"start_sec\", \"end_sec\"\n",
    "    ]\n",
    "    df = df.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "    prev_pid   = df[\"participant_id\"].shift(1)\n",
    "    prev_date  = df[\"session_date\"].shift(1)\n",
    "    prev_rec   = df[\"recording_id\"].shift(1)\n",
    "    prev_wav   = df[\"wav_path\"].shift(1)\n",
    "    prev_chunk = df[\"chunk_id\"].shift(1)\n",
    "    prev_spk   = df[\"speaker_id_local\"].shift(1)\n",
    "    prev_end   = df[\"end_sec\"].shift(1)\n",
    "\n",
    "    gap = df[\"start_sec\"] - prev_end\n",
    "\n",
    "    # define when a new merged segment must start\n",
    "    new_group = (\n",
    "        (df[\"participant_id\"] != prev_pid) |\n",
    "        (df[\"session_date\"] != prev_date) |\n",
    "        (df[\"recording_id\"] != prev_rec) |\n",
    "        (df[\"wav_path\"] != prev_wav) |\n",
    "        (df[\"chunk_id\"] != prev_chunk) |\n",
    "        (df[\"speaker_id_local\"] != prev_spk) |\n",
    "        (gap.isna()) |\n",
    "        (gap < 0) |\n",
    "        (gap > gap_thresh)\n",
    "    )\n",
    "\n",
    "    df[\"_merge_group\"] = new_group.cumsum()\n",
    "\n",
    "    # aggregate merged segments\n",
    "    out = (\n",
    "        df.groupby(\"_merge_group\", as_index=False)\n",
    "          .agg(\n",
    "              participant_id=(\"participant_id\", \"first\"),\n",
    "              session_date=(\"session_date\", \"first\"),\n",
    "              recording_id=(\"recording_id\", \"first\"),\n",
    "              wav_path=(\"wav_path\", \"first\"),\n",
    "              chunk_id=(\"chunk_id\", \"first\"),\n",
    "              speaker_id_local=(\"speaker_id_local\", \"first\"),\n",
    "              start_sec=(\"start_sec\", \"min\"),\n",
    "              end_sec=(\"end_sec\", \"max\"),\n",
    "              n_segments=(\"_orig_row\", \"count\"),\n",
    "              orig_rows=(\"_orig_row\", lambda x: list(x))\n",
    "          )\n",
    "    )\n",
    "\n",
    "    out[\"duration_sec\"] = out[\"end_sec\"] - out[\"start_sec\"]\n",
    "\n",
    "    out = (\n",
    "        out\n",
    "        .sort_values(\n",
    "            [\"participant_id\", \"session_date\", \"recording_id\", \"chunk_id\", \"start_sec\"]\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d81f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge_close_segments(final_df_full, gap_thresh=0.7)\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f879e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e8a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "m = joblib.load(\"models/xgb_egemaps.pkl\")\n",
    "print(type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32916e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "import opensmile\n",
    "\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38748e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"models/xgb_egemaps.pkl\")\n",
    "\n",
    "assert MODEL_PATH.exists(), f\"Model not found: {MODEL_PATH}\"\n",
    "xgb_model = joblib.load(MODEL_PATH)\n",
    "\n",
    "print(\"Loaded model:\", type(xgb_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ===== Class mapping (match training) =====\n",
    "LABEL2ID = {\n",
    "    \"adult_male\": 0,\n",
    "    \"adult_female\": 1,\n",
    "    \"child\": 2,\n",
    "    \"background\": 3,\n",
    "}\n",
    "\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "# Must follow training order\n",
    "CLASS_NAMES = [ID2LABEL[i] for i in range(len(ID2LABEL))]\n",
    "print(\"CLASS_NAMES:\", CLASS_NAMES)\n",
    "\n",
    "# ===== Model sanity check =====\n",
    "egemaps_dim = 88  # EGEMAPS feature dimension (match training)\n",
    "\n",
    "dummy_X = np.zeros((1, egemaps_dim), dtype=np.float32)\n",
    "proba = xgb_model.predict_proba(dummy_X)\n",
    "\n",
    "print(\"predict_proba shape:\", proba.shape)\n",
    "\n",
    "assert proba.shape[1] == len(CLASS_NAMES), (\n",
    "    f\"Model returns {proba.shape[1]} classes, \"\n",
    "    f\"but CLASS_NAMES has {len(CLASS_NAMES)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = [\"adult_male\", \"adult_female\", \"child\", \"background\"]\n",
    "N_CLASSES = len(CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "def load_audio_mono(path: str | Path) -> Tuple[np.ndarray, int]:\n",
    "    x, sr = sf.read(str(path), always_2d=False)\n",
    "    if x.ndim == 2:\n",
    "        x = x.mean(axis=1)\n",
    "    x = x.astype(np.float32, copy=False)\n",
    "    return x, sr\n",
    "\n",
    "def resample_audio(x: np.ndarray, sr: int, target_sr: int) -> np.ndarray:\n",
    "    if sr == target_sr:\n",
    "        return x\n",
    "    gcd = np.gcd(sr, target_sr)\n",
    "    up = target_sr // gcd\n",
    "    down = sr // gcd\n",
    "    return resample_poly(x, up, down).astype(np.float32, copy=False)\n",
    "\n",
    "def crop_or_pad(x: np.ndarray, target_len: int) -> np.ndarray:\n",
    "    n = len(x)\n",
    "    if n == target_len:\n",
    "        return x\n",
    "    if n > target_len:\n",
    "        return x[:target_len]\n",
    "    out = np.zeros(target_len, dtype=np.float32)\n",
    "    out[:n] = x\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4931713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_windows(start: float, end: float, win: float = 1.0, hop: float = 0.5) -> List[Tuple[float, float, float]]:\n",
    "    \"\"\"\n",
    "    Returns list of (w_start, w_end, weight_duration).\n",
    "    Short segments get a single padded window (start -> start+win).\n",
    "    Long segments use fixed 1.0s windows with 0.5s hop, plus an end-anchored last window if needed.\n",
    "    \"\"\"\n",
    "    dur = end - start\n",
    "\n",
    "    # too short -> single padded window\n",
    "    if dur <= 0 or dur < win:\n",
    "        return [(start, start + win, win)]\n",
    "\n",
    "    windows = []\n",
    "    t = start\n",
    "    while t + win <= end:\n",
    "        windows.append((t, t + win, win))\n",
    "        t += hop\n",
    "\n",
    "    # End-anchored last window if needed\n",
    "    if not windows or windows[-1][1] < end:\n",
    "        windows.append((end - win, end, win))\n",
    "\n",
    "    return windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75615a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EGemapsExtractor:\n",
    "    egemaps_dim: int = 88          \n",
    "    target_sr: int = 16000\n",
    "    win_sec: float = 1.0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.smile = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "\n",
    "    def _fix_dim(self, vec: np.ndarray) -> np.ndarray:\n",
    "        vec = vec.astype(np.float32).flatten()\n",
    "        if vec.shape[0] == self.egemaps_dim:\n",
    "            return vec\n",
    "        out = np.zeros(self.egemaps_dim, dtype=np.float32)\n",
    "        m = min(self.egemaps_dim, vec.shape[0])\n",
    "        out[:m] = vec[:m]\n",
    "        return out\n",
    "\n",
    "# {wav_path: (audio_16k, sr_16k)}\n",
    "_AUDIO_CACHE: Dict[str, Tuple[np.ndarray, int]] = {}\n",
    "\n",
    "def load_audio_16k_cached(wav_path: str | Path, target_sr: int = 16000) -> Tuple[np.ndarray, int]:\n",
    "    key = str(wav_path)\n",
    "    if key in _AUDIO_CACHE:\n",
    "        return _AUDIO_CACHE[key]\n",
    "\n",
    "    x, sr = load_audio_mono(key)\n",
    "    x = resample_audio(x, sr, target_sr)\n",
    "    _AUDIO_CACHE[key] = (x, target_sr)\n",
    "    return x, target_sr\n",
    "\n",
    "def extract_egemaps_for_window(\n",
    "    extractor: EGemapsExtractor,\n",
    "    wav_path: str | Path,\n",
    "    start_sec: float,\n",
    "    end_sec: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract eGeMAPS vector from [start_sec, end_sec] but always pad/crop to win_sec.\n",
    "    \"\"\"\n",
    "    x16, sr = load_audio_16k_cached(wav_path, extractor.target_sr)\n",
    "\n",
    "    s = int(round(start_sec * sr))\n",
    "    e = int(round(end_sec * sr))\n",
    "    s = max(0, s)\n",
    "    e = min(len(x16), e)\n",
    "\n",
    "    seg = x16[s:e]\n",
    "    target_len = int(sr * extractor.win_sec)\n",
    "    seg = crop_or_pad(seg, target_len)\n",
    "\n",
    "    try:\n",
    "        feats = extractor.smile.process_signal(seg, sr)\n",
    "        vec = feats.values.flatten()\n",
    "        return extractor._fix_dim(vec)\n",
    "    except Exception:\n",
    "        return np.zeros(extractor.egemaps_dim, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_probs(P: np.ndarray, weights: List[float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    P: (nwin, C) probabilities\n",
    "    weights: list length nwin\n",
    "    \"\"\"\n",
    "    W = np.array(weights, dtype=np.float32).reshape(-1, 1)\n",
    "    return (P * W).sum(axis=0) / (W.sum() + 1e-12)\n",
    "\n",
    "def predict_segment_probs(\n",
    "    row: pd.Series,\n",
    "    model,\n",
    "    extractor: EGemapsExtractor,\n",
    "    win: float = 1.0,\n",
    "    hop: float = 0.5\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Returns dict with:\n",
    "      - probs (C,)\n",
    "      - n_windows\n",
    "      - window_durations\n",
    "    \"\"\"\n",
    "    windows = generate_windows(float(row.start_sec), float(row.end_sec), win=win, hop=hop)\n",
    "\n",
    "    X_list = []\n",
    "    weights = []\n",
    "    for ws, we, wdur in windows:\n",
    "        vec = extract_egemaps_for_window(extractor, row.wav_path, ws, we)\n",
    "        X_list.append(vec)\n",
    "        weights.append(wdur)\n",
    "\n",
    "    Xw = np.stack(X_list, axis=0).astype(np.float32)       # (nwin, D)\n",
    "    Pw = model.predict_proba(Xw).astype(np.float32)         # (nwin, C)\n",
    "\n",
    "    p_final = weighted_mean_probs(Pw, weights)              # (C,)\n",
    "\n",
    "    return {\n",
    "        \"probs\": p_final,\n",
    "        \"n_windows\": len(windows),\n",
    "        \"window_durations\": weights,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bacbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = merged_df.copy()\n",
    "\n",
    "# sanity check required columns\n",
    "required_cols = [\"wav_path\", \"start_sec\", \"end_sec\", \"duration_sec\"]\n",
    "missing = [c for c in required_cols if c not in df_in.columns]\n",
    "assert not missing, f\"Missing columns in df: {missing}\"\n",
    "\n",
    "extractor = EGemapsExtractor(egemaps_dim=88, target_sr=16000, win_sec=1.0)\n",
    "\n",
    "probs_out = []\n",
    "nwin_out = []\n",
    "wdur_out = []\n",
    "\n",
    "for _, r in tqdm(df_in.iterrows(), total=len(df_in), desc=\"Classifying segments\"):\n",
    "    out = predict_segment_probs(r, xgb_model, extractor, win=1.0, hop=0.5)\n",
    "    probs_out.append(out[\"probs\"])\n",
    "    nwin_out.append(out[\"n_windows\"])\n",
    "    wdur_out.append(out[\"window_durations\"])\n",
    "\n",
    "P = np.vstack(probs_out)  # (N, C)\n",
    "\n",
    "df_out = df_in.copy()\n",
    "df_out[\"n_windows\"] = nwin_out\n",
    "df_out[\"window_durations\"] = wdur_out\n",
    "\n",
    "for i, cname in enumerate(CLASS_NAMES):\n",
    "    df_out[f\"probs_{cname}\"] = P[:, i].astype(float)\n",
    "\n",
    "pred_idx = np.argmax(P, axis=1)\n",
    "df_out[\"predicted_class\"] = [CLASS_NAMES[i] for i in pred_idx]\n",
    "df_out[\"predicted_confidence\"] = P[np.arange(len(df_out)), pred_idx].astype(float)\n",
    "\n",
    "df_out.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d171ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from scipy.signal import resample_poly\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "def _load_mono(path):\n",
    "    x, sr = sf.read(str(path), always_2d=False)\n",
    "    if x.ndim == 2:\n",
    "        x = x.mean(axis=1)\n",
    "    return x.astype(np.float32, copy=False), sr\n",
    "\n",
    "def _resample(x, sr, target_sr):\n",
    "    if sr == target_sr:\n",
    "        return x\n",
    "    g = np.gcd(sr, target_sr)\n",
    "    up = target_sr // g\n",
    "    down = sr // g\n",
    "    return resample_poly(x, up, down).astype(np.float32, copy=False)\n",
    "\n",
    "def _slice_audio(x, sr, start_sec, end_sec):\n",
    "    s = max(0, int(round(start_sec * sr)))\n",
    "    e = min(len(x), int(round(end_sec * sr)))\n",
    "    return x[s:e]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_audio(\n",
    "    df: pd.DataFrame,\n",
    "    target_sr: int = 16000,\n",
    "    gap_sec: float = 0.20,\n",
    "    max_total_sec: float = None,     # prevent huge memory usage; set None for no limit\n",
    "    min_conf: float = 0.0,\n",
    "    sort_by_time: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    df must have: wav_path, start_sec, end_sec\n",
    "    Returns: (audio_array, sr, used_rows_df)\n",
    "    \"\"\"\n",
    "    use_df = df.copy()\n",
    "\n",
    "    if min_conf is not None and \"predicted_confidence\" in use_df.columns:\n",
    "        use_df = use_df[use_df[\"predicted_confidence\"] >= float(min_conf)]\n",
    "\n",
    "    if sort_by_time:\n",
    "        use_df = use_df.sort_values([\"wav_path\", \"start_sec\"])\n",
    "\n",
    "    if use_df.empty:\n",
    "        raise ValueError(\"No rows selected to build combined audio.\")\n",
    "\n",
    "    gap = np.zeros(int(target_sr * gap_sec), dtype=np.float32) if gap_sec and gap_sec > 0 else None\n",
    "\n",
    "    pieces = []\n",
    "    total = 0.0\n",
    "    used_rows = []\n",
    "\n",
    "    for wav_path, gdf in use_df.groupby(\"wav_path\"):\n",
    "        x, sr = _load_mono(wav_path)\n",
    "        x = _resample(x, sr, target_sr)\n",
    "\n",
    "        for _, r in gdf.iterrows():\n",
    "            seg = _slice_audio(x, target_sr, float(r.start_sec), float(r.end_sec))\n",
    "            if len(seg) == 0:\n",
    "                continue\n",
    "\n",
    "            seg_dur = len(seg) / target_sr\n",
    "            if max_total_sec is not None and (total + seg_dur) > float(max_total_sec):\n",
    "                break\n",
    "\n",
    "            pieces.append(seg)\n",
    "            if gap is not None:\n",
    "                pieces.append(gap)\n",
    "\n",
    "            total += seg_dur + (gap_sec if gap is not None else 0.0)\n",
    "            used_rows.append(r)\n",
    "\n",
    "        if max_total_sec is not None and total >= float(max_total_sec):\n",
    "            break\n",
    "\n",
    "    if not pieces:\n",
    "        raise ValueError(\"No audio collected (all segments empty or max_total_sec too small).\")\n",
    "\n",
    "    y = np.concatenate(pieces).astype(np.float32, copy=False)\n",
    "    used_rows_df = pd.DataFrame(used_rows)\n",
    "\n",
    "    print(f\"Combined audio duration ≈ {len(y)/target_sr:.1f}s  | segments used = {len(used_rows_df)}\")\n",
    "    return y, target_sr, used_rows_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = df_out[df_out[\"predicted_class\"] == \"adult_female\"].copy()\n",
    "y, sr, used = build_combined_audio(sel, gap_sec=0.15, max_total_sec=None, min_conf=0.0)\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = df_out[df_out[\"predicted_class\"] == \"child\"].copy()\n",
    "y, sr, used = build_combined_audio(sel, gap_sec=0.15, max_total_sec=None, min_conf=0.0)\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ce0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = df_out[df_out[\"predicted_class\"] == \"background\"].copy()\n",
    "y, sr, used = build_combined_audio(sel, gap_sec=0.15, max_total_sec=None, min_conf=0.0)\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e36d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = df_out[df_out[\"predicted_class\"] == \"adult_male\"].copy()\n",
    "y, sr, used = build_combined_audio(sel, gap_sec=0.15, max_total_sec=None, min_conf=0.0)\n",
    "display(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeec084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a9ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HindiBabyNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
